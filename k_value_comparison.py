#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
k_value_comparison.py - Kå€¼å¯¹æ¯”åˆ†æè„šæœ¬

å¯¹æ¯ä¸ªäº‹ä»¶ï¼Œä½¿ç”¨K=0åˆ°5çš„ä¸åŒé‚»åŸŸå¤§å°è¿›è¡ŒLLMåˆ†æï¼Œ
æ¯”è¾ƒtokenæ¶ˆè€—å’Œåœ°å€è¯†åˆ«å‡†ç¡®æ€§çš„å˜åŒ–
"""

import os
import json
import pandas as pd
from typing import Dict, List, Any
from datetime import datetime
import logging

from contextual_path_analyzer import ContextualPathAnalyzer
from llm_analyzer import LLMAnalyzer

logger = logging.getLogger(__name__)


class KValueComparisonAnalyzer:
    """Kå€¼å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self, k_paths: int = 30, k_range: List[int] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            k_paths: é€‰æ‹©çš„topè·¯å¾„æ•°é‡
            k_range: Kå€¼èŒƒå›´ï¼Œé»˜è®¤[0,1,2,3,4,5]
        """
        self.k_paths = k_paths
        self.k_range = k_range or [0, 1, 2, 3, 4, 5]
        
        # åˆå§‹åŒ–LLMåˆ†æå™¨
        self.llm_analyzer = LLMAnalyzer()
        
        logger.info(f"Kå€¼å¯¹æ¯”åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - Top-Kè·¯å¾„: {k_paths}")
        logger.info(f"  - Kå€¼èŒƒå›´: {self.k_range}")
    

    

    
    def run_full_comparison(self, input_dir: str, output_dir: str) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„Kå€¼å¯¹æ¯”åˆ†æ
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        logger.info("="*80)
        logger.info("å¼€å§‹Kå€¼å¯¹æ¯”åˆ†æ")
        logger.info("="*80)
        
        # ä¸ºæ¯ä¸ªKå€¼éƒ½è¿›è¡Œç‹¬ç«‹çš„è·¯å¾„æå–å’Œä¸Šä¸‹æ–‡æ„å»º
        all_k_path_results = {}
        
        for k in self.k_range:
            logger.info(f"ä¸ºK={k}æ„å»ºç‹¬ç«‹çš„è·¯å¾„ä¸Šä¸‹æ–‡...")
            
            path_analyzer = ContextualPathAnalyzer(
                k_paths=self.k_paths,
                k_neighbors=k
            )
            
            # è·å–Kå€¼å¯¹åº”çš„è·¯å¾„ä¸Šä¸‹æ–‡
            k_path_results = path_analyzer.analyze_with_context(input_dir)
            all_k_path_results[k] = k_path_results
            
            if k_path_results['path_contexts']:
                total_contexts = sum(len(contexts) for contexts in k_path_results['path_contexts'].values())
                logger.info(f"  K={k}: æå–åˆ° {total_contexts} ä¸ªè·¯å¾„ä¸Šä¸‹æ–‡")
            else:
                logger.warning(f"  K={k}: æœªæå–åˆ°è·¯å¾„ä¸Šä¸‹æ–‡")
        
        logger.info(f"âœ… æ‰€æœ‰Kå€¼è·¯å¾„æå–å®Œæˆ")
        
        # ä½¿ç”¨K=0çš„ç»“æœä½œä¸ºåŸºç¡€äº‹ä»¶åˆ—è¡¨ï¼ˆå› ä¸ºæ‰€æœ‰Kå€¼çš„äº‹ä»¶åº”è¯¥æ˜¯ç›¸åŒçš„ï¼‰
        base_events = list(all_k_path_results[0]['path_contexts'].keys()) if 0 in all_k_path_results else []
        
        # ä¸ºæ¯ä¸ªäº‹ä»¶è¿›è¡ŒKå€¼å¯¹æ¯”åˆ†æ
        event_results = {}
        total_tokens_by_k = {k: 0 for k in self.k_range}
        
        for event_name in base_events:
            logger.info(f"\nğŸ“Š å¤„ç†äº‹ä»¶: {event_name}")
            
            # ä¸ºå½“å‰äº‹ä»¶æ”¶é›†æ‰€æœ‰Kå€¼çš„ä¸Šä¸‹æ–‡
            event_k_contexts = {}
            for k in self.k_range:
                if k in all_k_path_results and event_name in all_k_path_results[k]['path_contexts']:
                    event_k_contexts[k] = all_k_path_results[k]['path_contexts'][event_name]
                else:
                    logger.warning(f"äº‹ä»¶ {event_name} åœ¨K={k}æ—¶æ²¡æœ‰ä¸Šä¸‹æ–‡æ•°æ®")
                    event_k_contexts[k] = {}
            
            # åˆ†ææ‰€æœ‰Kå€¼
            k_results = {}
            for k in self.k_range:
                if event_k_contexts[k]:
                    logger.info(f"  åˆ†æK={k}...")
                    try:
                        analysis_result = self.llm_analyzer.analyze_event_contexts(event_k_contexts[k], k)
                        k_results[k] = analysis_result
                        
                        if analysis_result['success']:
                            tokens = analysis_result['token_usage']['total_tokens']
                            attacker = analysis_result['identified_addresses']['attacker']
                            victim = analysis_result['identified_addresses']['victim']
                            logger.info(f"    âœ… K={k}: Tokens={tokens}, æ”»å‡»è€…={attacker[:10]}..., å—å®³è€…={victim[:10]}...")
                            total_tokens_by_k[k] += tokens
                        else:
                            logger.error(f"    âŒ K={k}: {analysis_result.get('error', 'Unknown error')}")
                    except Exception as e:
                        logger.error(f"    âŒ K={k} åˆ†æå¤±è´¥: {str(e)}")
                        k_results[k] = {
                            'success': False,
                            'error': str(e),
                            'k_neighbors': k,
                            'event_name': event_name
                        }
                else:
                    logger.warning(f"  è·³è¿‡K={k}ï¼ˆæ— ä¸Šä¸‹æ–‡æ•°æ®ï¼‰")
                    k_results[k] = {
                        'success': False,
                        'error': 'No context data',
                        'k_neighbors': k,
                        'event_name': event_name
                    }
            
            event_results[event_name] = k_results
        
        # æ•´ç†æœ€ç»ˆç»“æœ
        final_results = {
            'analysis_config': {
                'k_paths': self.k_paths,
                'k_range': self.k_range,
                'input_dir': input_dir,
                'timestamp': datetime.now().isoformat()
            },
            'all_k_path_results': all_k_path_results,  # åŒ…å«æ‰€æœ‰Kå€¼çš„è·¯å¾„æå–ç»“æœ
            'event_analysis_results': event_results,
            'global_statistics': {
                'total_events': len(event_results),
                'successful_events_by_k': {},
                'total_tokens_by_k': total_tokens_by_k,
                'avg_tokens_by_k': {}
            }
        }
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡
        for k in self.k_range:
            successful_count = sum(
                1 for event_results in event_results.values()
                if k in event_results and event_results[k].get('success', False)
            )
            final_results['global_statistics']['successful_events_by_k'][k] = successful_count
            
            if successful_count > 0:
                avg_tokens = total_tokens_by_k[k] / successful_count
                final_results['global_statistics']['avg_tokens_by_k'][k] = avg_tokens
            else:
                final_results['global_statistics']['avg_tokens_by_k'][k] = 0
        
        # ä¿å­˜ç»“æœ
        saved_files = self._save_comparison_results(final_results, output_dir)
        
        # ä¸ºæ¯ä¸ªäº‹ä»¶ä¿å­˜ç‹¬ç«‹çš„LLMåˆ†ææŠ¥å‘Š
        self._save_individual_event_reports(final_results, output_dir)
        
        # è¾“å‡ºæ‘˜è¦
        self._print_comparison_summary(final_results)
        
        logger.info(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        for file_type, file_path in saved_files.items():
            logger.info(f"  - {file_type}: {os.path.basename(file_path)}")
        
        return final_results
    
    def _save_comparison_results(self, results: Dict[str, Any], output_dir: str) -> Dict[str, str]:
        """ä¿å­˜å¯¹æ¯”åˆ†æç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        # 1. ä¿å­˜å®Œæ•´ç»“æœ
        full_file = os.path.join(output_dir, f"k_value_comparison_full_{timestamp}.json")
        with open(full_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        saved_files['full_results'] = full_file
        
        # 2. ä¿å­˜äº‹ä»¶çº§æ±‡æ€»
        event_summary = []
        for event_name, k_results in results['event_analysis_results'].items():
            for k, result in k_results.items():
                if result.get('success', False):
                    event_summary.append({
                        'event_name': event_name,
                        'k_neighbors': k,
                        'total_tokens': result['token_usage']['total_tokens'],
                        'prompt_tokens': result['token_usage']['prompt_tokens'],
                        'completion_tokens': result['token_usage']['completion_tokens'],
                        'attacker_address': result['identified_addresses']['attacker'],
                        'victim_address': result['identified_addresses']['victim'],
                        'num_paths': result['num_paths'],
                        'prompt_length': result['prompt_length']
                    })
        
        if event_summary:
            summary_df = pd.DataFrame(event_summary)
            summary_file = os.path.join(output_dir, f"k_value_event_summary_{timestamp}.csv")
            summary_df.to_csv(summary_file, index=False)
            saved_files['event_summary'] = summary_file
        
        # 3. ä¿å­˜Kå€¼ç»Ÿè®¡æ±‡æ€»
        k_stats = []
        for k in results['analysis_config']['k_range']:
            stats = results['global_statistics']
            k_stats.append({
                'k_neighbors': k,
                'successful_events': stats['successful_events_by_k'][k],
                'total_tokens': stats['total_tokens_by_k'][k],
                'avg_tokens_per_event': stats['avg_tokens_by_k'][k],
                'total_events': stats['total_events']
            })
        
        k_stats_df = pd.DataFrame(k_stats)
        k_stats_file = os.path.join(output_dir, f"k_value_statistics_{timestamp}.csv")
        k_stats_df.to_csv(k_stats_file, index=False)
        saved_files['k_statistics'] = k_stats_file
        
        # 4. ä¿å­˜åœ°å€è¯†åˆ«ç»“æœå¯¹æ¯”
        address_comparison = []
        for event_name, k_results in results['event_analysis_results'].items():
            event_data = {'event_name': event_name}
            
            for k in results['analysis_config']['k_range']:
                if k in k_results and k_results[k].get('success', False):
                    result = k_results[k]
                    event_data[f'attacker_k{k}'] = result['identified_addresses']['attacker']
                    event_data[f'victim_k{k}'] = result['identified_addresses']['victim']
                    event_data[f'tokens_k{k}'] = result['token_usage']['total_tokens']
                else:
                    event_data[f'attacker_k{k}'] = 'FAILED'
                    event_data[f'victim_k{k}'] = 'FAILED'
                    event_data[f'tokens_k{k}'] = 0
            
            address_comparison.append(event_data)
        
        if address_comparison:
            addr_df = pd.DataFrame(address_comparison)
            addr_file = os.path.join(output_dir, f"address_identification_comparison_{timestamp}.csv")
            addr_df.to_csv(addr_file, index=False)
            saved_files['address_comparison'] = addr_file
        
        return saved_files
    
    def _save_individual_event_reports(self, results: Dict[str, Any], output_dir: str):
        """ä¸ºæ¯ä¸ªäº‹ä»¶ä¿å­˜ç‹¬ç«‹çš„LLMåˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        reports_dir = os.path.join(output_dir, "individual_event_reports")
        os.makedirs(reports_dir, exist_ok=True)
        
        for event_name, k_results in results['event_analysis_results'].items():
            # æ¸…ç†äº‹ä»¶åç§°ä½œä¸ºæ–‡ä»¶å
            safe_event_name = "".join(c for c in event_name if c.isalnum() or c in ('_', '-')).strip()
            safe_event_name = safe_event_name[:50]  # é™åˆ¶é•¿åº¦
            
            # ä¸ºæ¯ä¸ªäº‹ä»¶åˆ›å»ºæŠ¥å‘Š
            event_report = {
                'event_name': event_name,
                'analysis_timestamp': timestamp,
                'k_value_results': {}
            }
            
            # æ•´ç†æ¯ä¸ªKå€¼çš„ç»“æœ
            for k, result in k_results.items():
                if result.get('success', False):
                    event_report['k_value_results'][f'k_{k}'] = {
                        'k_neighbors': k,
                        'token_usage': result['token_usage'],
                        'identified_addresses': result['identified_addresses'],
                        'llm_analysis': {
                            'raw_response': result['llm_response']['raw_content'],
                            'parsed_data': result['llm_response']['parsed_data']
                        },
                        'prompt_length': result['prompt_length'],
                        'num_paths': result['num_paths']
                    }
                else:
                    event_report['k_value_results'][f'k_{k}'] = {
                        'k_neighbors': k,
                        'error': result.get('error', 'Unknown error'),
                        'success': False
                    }
            
            # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
            json_file = os.path.join(reports_dir, f"{safe_event_name}_{timestamp}.json")
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(event_report, f, indent=2, ensure_ascii=False, default=str)
            
            # ä¿å­˜å¯è¯»æ–‡æœ¬æ ¼å¼çš„æŠ¥å‘Š
            txt_file = os.path.join(reports_dir, f"{safe_event_name}_{timestamp}.txt")
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(f"äº‹ä»¶åˆ†ææŠ¥å‘Š: {event_name}\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"åˆ†ææ—¶é—´: {timestamp}\n")
                f.write(f"Kå€¼èŒƒå›´: {list(k_results.keys())}\n\n")
                
                for k in sorted(k_results.keys()):
                    result = k_results[k]
                    f.write(f"--- K={k} åˆ†æç»“æœ ---\n")
                    
                    if result.get('success', False):
                        tokens = result['token_usage']
                        addresses = result['identified_addresses']
                        
                        f.write(f"çŠ¶æ€: âœ… æˆåŠŸ\n")
                        f.write(f"Tokenä½¿ç”¨: {tokens['total_tokens']} (è¾“å…¥:{tokens['prompt_tokens']}, è¾“å‡º:{tokens['completion_tokens']})\n")
                        f.write(f"æ”»å‡»è€…åœ°å€: {addresses['attacker']}\n")
                        f.write(f"å—å®³è€…åœ°å€: {addresses['victim']}\n")
                        f.write(f"è·¯å¾„æ•°é‡: {result['num_paths']}\n")
                        f.write(f"Prompté•¿åº¦: {result['prompt_length']} å­—ç¬¦\n\n")
                        
                        # æ·»åŠ LLMåˆ†æå†…å®¹ï¼ˆæˆªå–å‰500å­—ç¬¦ï¼‰
                        llm_content = result['llm_response']['raw_content']
                        f.write(f"LLMåˆ†æå†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰:\n")
                        f.write("-" * 40 + "\n")
                        f.write(f"{llm_content[:500]}...\n")
                        f.write("-" * 40 + "\n\n")
                    else:
                        f.write(f"çŠ¶æ€: âŒ å¤±è´¥\n")
                        f.write(f"é”™è¯¯: {result.get('error', 'Unknown error')}\n\n")
        
        logger.info(f"âœ… å·²ä¸º {len(results['event_analysis_results'])} ä¸ªäº‹ä»¶ä¿å­˜ç‹¬ç«‹æŠ¥å‘Šåˆ°: {reports_dir}")
    
    def _print_comparison_summary(self, results: Dict[str, Any]):
        """æ‰“å°å¯¹æ¯”åˆ†ææ‘˜è¦"""
        logger.info("\n" + "="*80)
        logger.info("Kå€¼å¯¹æ¯”åˆ†ææ‘˜è¦")
        logger.info("="*80)
        
        stats = results['global_statistics']
        total_events = stats['total_events']
        
        logger.info(f"æ€»äº‹ä»¶æ•°: {total_events}")
        logger.info(f"Kå€¼èŒƒå›´: {results['analysis_config']['k_range']}")
        
        logger.info(f"\nğŸ“Š å„Kå€¼ç»Ÿè®¡:")
        for k in results['analysis_config']['k_range']:
            successful = stats['successful_events_by_k'][k]
            total_tokens = stats['total_tokens_by_k'][k]
            avg_tokens = stats['avg_tokens_by_k'][k]
            
            logger.info(f"  K={k}: æˆåŠŸ={successful}/{total_events}, "
                       f"æ€»Tokens={total_tokens:,}, å¹³å‡Tokens={avg_tokens:.1f}")
        
        logger.info(f"\nğŸ“‹ åˆ†æå®Œæˆç»Ÿè®¡:")
        logger.info(f"  æˆåŠŸåˆ†æçš„äº‹ä»¶æ•°: {max(stats['successful_events_by_k'].values())}/{total_events}")
        logger.info(f"  æ€»Tokenæ¶ˆè€—: {sum(stats['total_tokens_by_k'].values()):,}")
        
        # æ˜¾ç¤ºæ¯ä¸ªäº‹ä»¶çš„Kå€¼åˆ†æç»“æœ
        logger.info(f"\nğŸ“Š å„äº‹ä»¶Kå€¼åˆ†æç»“æœ:")
        for event_name, k_results in results['event_analysis_results'].items():
            successful_k_values = [k for k, result in k_results.items() if result.get('success', False)]
            if successful_k_values:
                logger.info(f"  {event_name}: æˆåŠŸåˆ†æKå€¼ {successful_k_values}")
            else:
                logger.info(f"  {event_name}: âŒ æ‰€æœ‰Kå€¼åˆ†æå‡å¤±è´¥")


if __name__ == "__main__":
    # è¿è¡ŒKå€¼å¯¹æ¯”åˆ†æ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    input_dir = "/home/os/shuzheng/whole_pipeline/path_datasets_labeled"
    output_dir = "/home/os/shuzheng/whole_pipeline/RQ3/k_comparison_results"
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = KValueComparisonAnalyzer(
            k_paths=30,
            k_range=[0, 1, 2, 3, 4, 5]
        )
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        results = analyzer.run_full_comparison(input_dir, output_dir)
        
        print("\nğŸ‰ Kå€¼å¯¹æ¯”åˆ†æå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ Kå€¼å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()