#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆåŸºçº¿å¯¹æ¯”ç³»ç»Ÿï¼ˆåŒ…å«æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼‰

é›†æˆä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿å’Œæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œä½¿ç”¨ç»Ÿä¸€çš„LOGOäº¤å‰éªŒè¯è¿›è¡Œè¯„ä¼°ã€‚
"""

import os
import sys
import glob
import json
import numpy as np
import pandas as pd
import logging
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import LeaveOneGroupOut

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# å¯¼å…¥åŸºçº¿æ–¹æ³•
sys.path.append('/home/os/shuzheng/whole_pipeline')
from baseline_comparison import (
    DepthBasedMethod, RarityBasedMethod, SemanticBasedMethod, RandomMethod,
    MLBasedMethod, compute_attack_hit_rate_baseline, load_and_prepare_data
)

# æ£€æŸ¥PyTorchæ˜¯å¦å¯ç”¨
try:
    import torch
    from deep_learning_baselines import create_mlp_baseline, create_transformer_baseline
    PYTORCH_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info(f"PyTorchå¯ç”¨ï¼Œè®¾å¤‡: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}")
except ImportError:
    PYTORCH_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("PyTorchæœªå®‰è£…ï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ åŸºçº¿")

# æ£€æŸ¥å…¶ä»–ä¾èµ–
try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("Scikit-learnæœªå®‰è£…ï¼Œå°†è·³è¿‡ä¼ ç»ŸMLåŸºçº¿")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    logger.warning("XGBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡XGBooståŸºçº¿")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class UnifiedBaselineWrapper:
    """ç»Ÿä¸€çš„åŸºçº¿æ–¹æ³•åŒ…è£…å™¨"""
    
    def __init__(self, method, method_type='traditional'):
        self.method = method
        self.method_type = method_type
        self.name = getattr(method, 'name', str(method))
    
    def fit(self, X: pd.DataFrame, y: np.ndarray, groups: np.ndarray = None):
        """è®­ç»ƒæ¨¡å‹"""
        if self.method_type == 'deep_learning':
            self.method.fit(X, y, groups)
        else:
            # ä¼ ç»Ÿæ–¹æ³•
            self.method.fit(X, y, groups)
    
    def predict_scores(self, X: pd.DataFrame, groups: np.ndarray = None) -> np.ndarray:
        """é¢„æµ‹åˆ†æ•°"""
        return self.method.predict_scores(X, groups)


def evaluate_unified_method(wrapper: UnifiedBaselineWrapper, 
                          df: pd.DataFrame, 
                          groups: np.ndarray, 
                          k: int = 30) -> Dict[str, Any]:
    """ä½¿ç”¨LOGOè¯„ä¼°ç»Ÿä¸€åŸºçº¿æ–¹æ³•"""
    logger.info(f"è¯„ä¼°æ–¹æ³•: {wrapper.name} (ç±»å‹: {wrapper.method_type})")
    
    logo = LeaveOneGroupOut()
    per_file_results = {}
    y = df['label'].values
    
    for train_idx, test_idx in logo.split(df, y, groups):
        train_df = df.iloc[train_idx]
        test_df = df.iloc[test_idx]
        test_groups = groups[test_idx]
        
        # è·å–æµ‹è¯•æ–‡ä»¶å
        test_file = test_groups[0]
        
        logger.info(f"  è®­ç»ƒ: {len(train_df)} æ ·æœ¬, æµ‹è¯•: {test_file} ({len(test_df)} æ ·æœ¬)")
        
        try:
            # è®­ç»ƒæ¨¡å‹
            wrapper.fit(train_df, y[train_idx], groups[train_idx])
            
            # é¢„æµ‹æµ‹è¯•é›†
            test_scores = wrapper.predict_scores(test_df, test_groups)
            
            # è®¡ç®—æŒ‡æ ‡
            test_df_with_scores = test_df.copy()
            test_df_with_scores['score'] = test_scores
            
            file_metrics = compute_attack_hit_rate_baseline(test_df_with_scores, k)
            per_file_results[test_file] = file_metrics
            
            logger.info(f"    {test_file}: å‘½ä¸­ç‡={file_metrics['attack_hit_rate']:.3f}, "
                       f"F1={file_metrics['f1']:.3f}, é€‰æ‹©={file_metrics['actual_k']}/{file_metrics['total_paths_after_dedup']}")
            
        except Exception as e:
            logger.error(f"    {test_file} è¯„ä¼°å¤±è´¥: {e}")
            # æ·»åŠ é»˜è®¤ç»“æœé¿å…åç»­è®¡ç®—é”™è¯¯
            per_file_results[test_file] = {
                'attack_hit_rate': 0.0, 'precision': 0.0, 'recall': 0.0, 
                'accuracy': 0.0, 'f1': 0.0, 'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0,
                'total_attack_paths': 0, 'hit_attack_paths': 0, 'actual_k': 0,
                'total_paths_after_dedup': 0
            }
            continue
    
    # è®¡ç®—å…¨å±€æŒ‡æ ‡
    if per_file_results:
        # Macro-averaged
        valid_results = [r for r in per_file_results.values() if r['f1'] >= 0]
        if valid_results:
            macro_attack_hit_rate = np.mean([r['attack_hit_rate'] for r in valid_results])
            macro_precision = np.mean([r['precision'] for r in valid_results])
            macro_recall = np.mean([r['recall'] for r in valid_results])
            macro_f1 = np.mean([r['f1'] for r in valid_results])
            macro_accuracy = np.mean([r['accuracy'] for r in valid_results])
        else:
            macro_attack_hit_rate = macro_precision = macro_recall = macro_f1 = macro_accuracy = 0.0
        
        # Micro-averaged
        total_tp = sum(r['TP'] for r in per_file_results.values())
        total_fp = sum(r['FP'] for r in per_file_results.values())
        total_fn = sum(r['FN'] for r in per_file_results.values())
        total_tn = sum(r['TN'] for r in per_file_results.values())
        
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        micro_accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn) if (total_tp + total_tn + total_fp + total_fn) > 0 else 0.0
        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0
        
        # å…¨å±€æ”»å‡»è·¯å¾„å‘½ä¸­ç‡
        total_attacks = sum(r['total_attack_paths'] for r in per_file_results.values())
        total_hits = sum(r['hit_attack_paths'] for r in per_file_results.values())
        global_attack_hit_rate = total_hits / total_attacks if total_attacks > 0 else 0.0
    else:
        macro_attack_hit_rate = macro_precision = macro_recall = macro_f1 = macro_accuracy = 0.0
        micro_precision = micro_recall = micro_accuracy = micro_f1 = 0.0
        global_attack_hit_rate = 0.0
    
    return {
        'method_name': wrapper.name,
        'method_type': wrapper.method_type,
        'per_file_results': per_file_results,
        'macro_metrics': {
            'attack_hit_rate': macro_attack_hit_rate,
            'precision': macro_precision,
            'recall': macro_recall,
            'f1': macro_f1,
            'accuracy': macro_accuracy
        },
        'micro_metrics': {
            'attack_hit_rate': global_attack_hit_rate,
            'precision': micro_precision,
            'recall': micro_recall,
            'f1': micro_f1,
            'accuracy': micro_accuracy
        },
        'total_files': len(per_file_results)
    }


def main():
    """ä¸»å‡½æ•°"""
    input_dir = "/home/os/shuzheng/whole_pipeline/path_datasets_labeled"
    output_dir = f"/home/os/shuzheng/whole_pipeline/comprehensive_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    k = 20  # Top-30
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("=" * 60)
    logger.info("ç»¼åˆåŸºçº¿å¯¹æ¯”ç³»ç»Ÿå¯åŠ¨ï¼ˆåŒ…å«æ·±åº¦å­¦ä¹ ï¼‰")
    logger.info("=" * 60)
    logger.info(f"è¾“å…¥ç›®å½•: {input_dir}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"Top-K: {k}")
    
    # åŠ è½½æ•°æ®
    try:
        df, groups = load_and_prepare_data(input_dir)
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return 1
    
    # æ”¶é›†æ‰€æœ‰åŸºçº¿æ–¹æ³•
    methods = []
    
    # 1. ç»å…¸å•ç‰¹å¾åŸºçº¿
    methods.extend([
        UnifiedBaselineWrapper(DepthBasedMethod(), 'traditional'),
        UnifiedBaselineWrapper(RarityBasedMethod(), 'traditional'),
        UnifiedBaselineWrapper(SemanticBasedMethod(), 'traditional'),
        UnifiedBaselineWrapper(RandomMethod(), 'traditional'),
    ])
    
    # 2. ä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºçº¿
    if SKLEARN_AVAILABLE:
        methods.extend([
            UnifiedBaselineWrapper(
                MLBasedMethod("Random Forest (Enhanced)", 
                             RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, class_weight='balanced'),
                             use_advanced_features=True), 'ml_enhanced'),
            UnifiedBaselineWrapper(
                MLBasedMethod("Logistic Regression (Enhanced)", 
                             LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),
                             use_tfidf=True, use_advanced_features=True), 'ml_enhanced'),
        ])
    
    # 3. XGBooståŸºçº¿
    if XGBOOST_AVAILABLE:
        n_positive = (df['label'] == 1).sum()
        n_negative = len(df) - n_positive
        scale_pos_weight = n_negative / n_positive if n_positive > 0 else 1.0
        
        methods.append(
            UnifiedBaselineWrapper(
                MLBasedMethod("XGBoost (Enhanced)", 
                             xgb.XGBClassifier(n_estimators=300, max_depth=8, learning_rate=0.05,
                                             random_state=42, scale_pos_weight=scale_pos_weight),
                             use_advanced_features=True), 'ml_enhanced')
        )
    
    # 4. æ·±åº¦å­¦ä¹ åŸºçº¿
    if PYTORCH_AVAILABLE:
        # MLP + EmbeddingåŸºçº¿
        mlp_baseline = create_mlp_baseline(
            model_params={'embedding_dim': 64, 'hidden_dim': 128, 'dropout': 0.3},
            training_params={'batch_size': 32, 'learning_rate': 1e-3, 'num_epochs': 30}
        )
        methods.append(UnifiedBaselineWrapper(mlp_baseline, 'deep_learning'))
        
        # TransformeråŸºçº¿
        transformer_baseline = create_transformer_baseline(
            model_params={'embedding_dim': 64, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.3},
            training_params={'batch_size': 16, 'learning_rate': 5e-4, 'num_epochs': 40}
        )
        methods.append(UnifiedBaselineWrapper(transformer_baseline, 'deep_learning'))
    
    logger.info(f"å°†è¯„ä¼° {len(methods)} ç§åŸºçº¿æ–¹æ³•")
    
    # è¯„ä¼°æ‰€æœ‰æ–¹æ³•
    all_results = {}
    
    for i, wrapper in enumerate(methods, 1):
        logger.info(f"\n[{i}/{len(methods)}] å¼€å§‹è¯„ä¼°: {wrapper.name}")
        try:
            result = evaluate_unified_method(wrapper, df, groups, k)
            all_results[wrapper.name] = result
            
            # è¾“å‡ºå½“å‰ç»“æœ
            macro_hit_rate = result['macro_metrics']['attack_hit_rate']
            macro_f1 = result['macro_metrics']['f1']
            logger.info(f"âœ… {wrapper.name} å®Œæˆ: å®å¹³å‡å‘½ä¸­ç‡={macro_hit_rate:.3f}, å®å¹³å‡F1={macro_f1:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ {wrapper.name} è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(output_dir, "comprehensive_results.json")
    with open(results_file, 'w') as f:
        json.dump(all_results, f, indent=2, default=str)
    
    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    summary_data = []
    for method_name, result in all_results.items():
        summary_data.append({
            'Method': method_name,
            'Type': result['method_type'],
            'Macro_Attack_Hit_Rate': f"{result['macro_metrics']['attack_hit_rate']:.3f}",
            'Macro_Precision': f"{result['macro_metrics']['precision']:.3f}",
            'Macro_Recall': f"{result['macro_metrics']['recall']:.3f}",
            'Macro_F1': f"{result['macro_metrics']['f1']:.3f}",
            'Micro_Attack_Hit_Rate': f"{result['micro_metrics']['attack_hit_rate']:.3f}",
            'Micro_F1': f"{result['micro_metrics']['f1']:.3f}",
            'Files_Evaluated': result['total_files']
        })
    
    # æŒ‰å®å¹³å‡å‘½ä¸­ç‡æ’åº
    summary_data.sort(key=lambda x: float(x['Macro_Attack_Hit_Rate']), reverse=True)
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = os.path.join(output_dir, "comprehensive_summary.csv")
    summary_df.to_csv(summary_file, index=False)
    
    # è¾“å‡ºæœ€ç»ˆæ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ç»¼åˆåŸºçº¿å¯¹æ¯”å®Œæˆ")
    logger.info("=" * 80)
    logger.info("æ’å (æŒ‰å®å¹³å‡æ”»å‡»è·¯å¾„å‘½ä¸­ç‡):")
    
    for i, row in summary_df.iterrows():
        method_type = row['Type']
        type_icon = {"traditional": "ğŸ“Š", "ml_enhanced": "ğŸ¤–", "deep_learning": "ğŸ§ "}.get(method_type, "â“")
        logger.info(f"{i+1:2d}. {type_icon} {row['Method']:25s} | å‘½ä¸­ç‡: {row['Macro_Attack_Hit_Rate']} | F1: {row['Macro_F1']}")
    
    # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºæœ€ä¼˜ç»“æœ
    logger.info("\næŒ‰æ–¹æ³•ç±»å‹æœ€ä¼˜ç»“æœ:")
    type_groups = summary_df.groupby('Type')
    for method_type, group in type_groups:
        best_method = group.iloc[0]
        type_icon = {"traditional": "ğŸ“Š", "ml_enhanced": "ğŸ¤–", "deep_learning": "ğŸ§ "}.get(method_type, "â“")
        logger.info(f"{type_icon} {method_type:15s}: {best_method['Method']:25s} | å‘½ä¸­ç‡: {best_method['Macro_Attack_Hit_Rate']}")
    
    logger.info(f"\nè¯¦ç»†ç»“æœä¿å­˜åœ¨: {output_dir}")
    logger.info(f"æ±‡æ€»è¡¨æ ¼: {summary_file}")
    logger.info(f"è¯¦ç»†æ•°æ®: {results_file}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())